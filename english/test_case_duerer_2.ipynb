{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Wikipedia Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils_wiki import get_wikipedia_article, save_wikipedia_page\n",
    "\n",
    "person_name = \"Albrecht Duerer\"\n",
    "\n",
    "\n",
    "if not os.path.exists(\"data/wikipedia\"): os.makedirs(\"data/wikipedia\")\n",
    "if not os.path.exists(\"data/json\"): os.makedirs(\"data/json\")\n",
    "\n",
    "text_filename = f\"data/wikipedia/{person_name.replace(' ', '_').lower()}.txt\"\n",
    "json_nlp_filename = f\"data/json/{person_name.replace(' ', '_').lower()}.json\"\n",
    "\n",
    "if not os.path.exists(text_filename):\n",
    "    wiki_page = get_wikipedia_article(person_name)\n",
    "    if wiki_page:\n",
    "        print(f\"Found a Page: {wiki_page.title}\")\n",
    "        text = wiki_page.content\n",
    "        save_wikipedia_page(wiki_page, text_filename, include_metadata=True)\n",
    "    else:\n",
    "        print(f\"Query Failed! Couldn't find {person_name}\")\n",
    "else:\n",
    "    text = open(text_filename).read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean & Pre-process Text (SpaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albrecht Dürer (; German: [ˈʔalbʁɛçt ˈdyːʁɐ]; Hungarian: Ajtósi Adalbert; 21 May 1471 – 6 April 1528\n",
      "dict_keys(['text', 'morphology', 'entities', 'time_expressions', 'semantic_roles', 'coreference', 'tokenization', 'relations', 'linked_entities'])\n",
      "{'ID': 'ent_0_0', 'sentenceID': 0, 'surfaceForm': 'Albrecht Dürer', 'category': 'PERSON', 'locationStart': 0, 'locationEnd': 14, 'tokenStart': 0, 'tokenEnd': 2, 'score': 0.9976517856121063, 'method': 'flair_ner-ontonotes-large_0.12.2'}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp_common import create_nlp_template, add_morphosyntax, run_spacy\n",
    "import spacy\n",
    "from spacy import __version__ as spacy_version\n",
    "\n",
    "\n",
    "with open(text_filename) as f:\n",
    "    text = f.read()\n",
    "    text = text[:5000]\n",
    "    # text = preprocess_and_clean_text(text)\n",
    "    nlp_dict, is_from_file = create_nlp_template(text, filepath=json_nlp_filename)\n",
    "\n",
    "# NLP Basic processing using SpaCy (Only if file did not exist already)\n",
    "if not is_from_file:\n",
    "    spacy_model = \"en_core_web_sm\"\n",
    "    nlp = spacy.load(spacy_model)\n",
    "    spacy_dict = run_spacy(text, nlp)\n",
    "    nlp_dict['tokenization'] = {f'spacy_{spacy_model}_{spacy_version}': spacy_dict['tokens']}\n",
    "    nlp_dict['morphology'] = {f'spacy_{spacy_model}_{spacy_version}': add_morphosyntax(spacy_dict['token_objs'])}\n",
    "else:\n",
    "    text = nlp_dict['text']\n",
    "\n",
    "print(text[:100])\n",
    "print(nlp_dict.keys())\n",
    "print(nlp_dict[\"entities\"][0])\n",
    "print(is_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Flair Libraries\n",
    "from flair import __version__ as flair_version\n",
    "from flair.splitter import SegtokSentenceSplitter\n",
    "from utils_nlp_flair import run_flair, add_morphosyntax_flair\n",
    "\n",
    "splitter = SegtokSentenceSplitter()\n",
    "flair_models = {\n",
    "    \"chunker\": \"chunk\",\n",
    "    \"ner\": 'ner-ontonotes-large', # \n",
    "    \"relations\": \"relations\", # If relations is provided then is not necessary to do NER sepparately!\n",
    "    \"frames\": \"frame\",\n",
    "    \"linker\": \"linker\"\n",
    "}\n",
    "\n",
    "morpho, tokenized_doc = add_morphosyntax_flair(text, splitter)\n",
    "\n",
    "nlp_dict['tokenization'][f\"flair_{flair_version}\"] = tokenized_doc\n",
    "nlp_dict['morphology'][f\"flair_{flair_version}\"] = morpho"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Named Entities & Relations (Flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-15 15:47:13,904 SequenceTagger predicts: Dictionary with 76 tags: <unk>, O, B-CARDINAL, E-CARDINAL, S-PERSON, S-CARDINAL, S-PRODUCT, B-PRODUCT, I-PRODUCT, E-PRODUCT, B-WORK_OF_ART, I-WORK_OF_ART, E-WORK_OF_ART, B-PERSON, E-PERSON, S-GPE, B-DATE, I-DATE, E-DATE, S-ORDINAL, S-LANGUAGE, I-PERSON, S-EVENT, S-DATE, B-QUANTITY, E-QUANTITY, S-TIME, B-TIME, I-TIME, E-TIME, B-GPE, E-GPE, S-ORG, I-GPE, S-NORP, B-FAC, I-FAC, E-FAC, B-NORP, E-NORP, S-PERCENT, B-ORG, E-ORG, B-LANGUAGE, E-LANGUAGE, I-CARDINAL, I-ORG, S-WORK_OF_ART, I-QUANTITY, B-MONEY\n"
     ]
    }
   ],
   "source": [
    "sentences = splitter.split(text)\n",
    "\n",
    "if 'entities' not in nlp_dict: nlp_dict['entities'] = []\n",
    "if 'relations' not in nlp_dict: nlp_dict['relations'] = []\n",
    "\n",
    "ent_rel_out = run_flair(sentences, \"relations\", flair_models)\n",
    "nlp_dict['entities'] = ent_rel_out[\"tagged_entities\"]\n",
    "nlp_dict['relations'] = ent_rel_out[\"tagged_relations\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Linked Entities (Flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-15 15:49:08,980 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "# Must restart the sentence to erase previous tags\n",
    "sentences = splitter.split(text)\n",
    "if 'linked_entities' not in nlp_dict: nlp_dict['linked_entities'] = []\n",
    "nlp_dict['linked_entities'] = run_flair(sentences, \"linker\", flair_models, metadata={\"entity_ids\":ent_rel_out[\"entity_ids\"]})[\"tagged_entities\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Predicate Senses & Merge with SRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_dict['semantic_roles'] = run_flair(sentences, \"frames\", flair_models)[\"tagged_entities\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save File Appending the new Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "intavia_dict = {\n",
    "            'status': '200',\n",
    "            'data': nlp_dict\n",
    "        }\n",
    "\n",
    "json.dump(intavia_dict, open(json_nlp_filename, \"w\"), indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
