{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Wikipedia Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: {'Deutsche Schule Sevilla', 'Albrecht Dürer', 'Goslar'}\n",
      "Ordered Options Compund Metric: [RankedArticle(wikipage_title='Albrecht Dürer', queried_name='Albrecht Duerer', lev_similarity=0.896551724137931, token_overlap=0.5, dates_confidence=-1)]\n",
      "\n",
      "Retrieving page for Albrecht Dürer\n",
      "Wiki Life Data = (1471 - 1528)\n",
      "Page Chosen! Confidence Score = 1\n",
      "Found a Page: Albrecht Dürer\n",
      "Albrecht Dürer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from utils.utils_wiki import get_wikipedia_article, save_wikipedia_page\n",
    "from utils.nlp_common import preprocess_and_clean_text\n",
    "\n",
    "person_name = \"Albrecht Duerer\"\n",
    "wikipedia_title = person_name\n",
    "\n",
    "\n",
    "if not os.path.exists(\"data/wikipedia\"): os.makedirs(\"data/wikipedia\")\n",
    "if not os.path.exists(\"data/json\"): os.makedirs(\"data/json\")\n",
    "\n",
    "\n",
    "wiki_page = get_wikipedia_article(person_name)\n",
    "if wiki_page:\n",
    "    print(f\"Found a Page: {wiki_page.title}\")\n",
    "    text = wiki_page.content\n",
    "    wikipedia_title = wiki_page.title\n",
    "    text_filename = f\"data/wikipedia/{wikipedia_title.replace(' ', '_').lower()}.txt\"\n",
    "    json_nlp_filename = f\"data/json/{wikipedia_title.replace(' ', '_').lower()}.json\"\n",
    "    save_wikipedia_page(wiki_page, text_filename, include_metadata=True, include_sections=True)\n",
    "else:\n",
    "    print(f\"Query Failed! Couldn't find {person_name}\")\n",
    "\n",
    "text = preprocess_and_clean_text(text)\n",
    "print(wikipedia_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from collections import OrderedDict\n",
    "from typing import  Dict, Optional, Any\n",
    "from utils.utils_wiki import get_wiki_linked_entities\n",
    "\n",
    "wikipedia_title = \"Albrecht Dürer\" # This title has to match EXACTLY the Wikipedia Article's name\n",
    "text_filename = f\"data/wikipedia/{wikipedia_title.replace(' ', '_').lower()}.txt\"\n",
    "json_nlp_filename = f\"data/json/{wikipedia_title.replace(' ', '_').lower()}.json\"\n",
    "\n",
    "if wikipedia_title:\n",
    "    raw_file = f\"data/wikipedia/{wikipedia_title.replace(' ', '_').lower()}.raw.txt\"\n",
    "    response = requests.get(f'https://en.wikipedia.org/wiki/{wikipedia_title}?action=raw')\n",
    "    raw_wiki = response.text\n",
    "    with open(raw_file, \"w\") as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "linked = get_wiki_linked_entities(raw_wiki)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean & Pre-process Text (SpaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albrecht Dürer (; German: [ˈʔalbʁɛçt ˈdyːʁɐ]; Hungarian: Ajtósi Adalbert; 21 May 1471 – 6 April 1528\n",
      "dict_keys(['text', 'tokenization', 'morphology', 'entities', 'time_expressions', 'semantic_roles', 'coreference'])\n",
      "[]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from utils.nlp_common import create_nlp_template, add_morphosyntax, run_spacy, preprocess_and_clean_text\n",
    "import spacy\n",
    "from spacy import __version__ as spacy_version\n",
    "\n",
    "\n",
    "with open(text_filename) as f:\n",
    "    text = f.read()\n",
    "    text = text[:5000]\n",
    "    text = preprocess_and_clean_text(text)\n",
    "    nlp_dict, is_from_file = create_nlp_template(text, filepath=json_nlp_filename)\n",
    "\n",
    "# NLP Basic processing using SpaCy (Only if file did not exist already)\n",
    "if not is_from_file:\n",
    "    spacy_model = \"en_core_web_sm\"\n",
    "    nlp = spacy.load(spacy_model)\n",
    "    spacy_dict = run_spacy(text, nlp)\n",
    "    nlp_dict['tokenization'] = {f'spacy_{spacy_model}_{spacy_version}': spacy_dict['tokens']}\n",
    "    nlp_dict['morphology'] = {f'spacy_{spacy_model}_{spacy_version}': add_morphosyntax(spacy_dict['token_objs'])}\n",
    "else:\n",
    "    text = nlp_dict['text']\n",
    "\n",
    "print(text[:100])\n",
    "print(nlp_dict.keys())\n",
    "print(nlp_dict[\"entities\"])\n",
    "print(is_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-26 17:02:50,991 SequenceTagger predicts: Dictionary with 76 tags: <unk>, O, B-CARDINAL, E-CARDINAL, S-PERSON, S-CARDINAL, S-PRODUCT, B-PRODUCT, I-PRODUCT, E-PRODUCT, B-WORK_OF_ART, I-WORK_OF_ART, E-WORK_OF_ART, B-PERSON, E-PERSON, S-GPE, B-DATE, I-DATE, E-DATE, S-ORDINAL, S-LANGUAGE, I-PERSON, S-EVENT, S-DATE, B-QUANTITY, E-QUANTITY, S-TIME, B-TIME, I-TIME, E-TIME, B-GPE, E-GPE, S-ORG, I-GPE, S-NORP, B-FAC, I-FAC, E-FAC, B-NORP, E-NORP, S-PERCENT, B-ORG, E-ORG, B-LANGUAGE, E-LANGUAGE, I-CARDINAL, I-ORG, S-WORK_OF_ART, I-QUANTITY, B-MONEY\n",
      "2023-06-26 17:02:56,163 SequenceTagger predicts: Dictionary with 4852 tags: <unk>, be.01, be.03, have.01, say.01, do.01, have.03, do.02, be.02, know.01, think.01, come.01, see.01, want.01, go.02, tell.01, give.01, use.01, make.02, take.01, talk.01, get.01, go.04, live.01, need.01, believe.01, work.01, mean.01, have.02, look.01, become.01, die.01, help.01, find.01, try.01, hear.01, know.06, show.01, happen.01, let.01, sell.01, bring.01, make.01, invest.01, begin.01, make.LV, continue.01, kill.01, speak.01, start.01\n",
      "2023-06-26 17:03:25,484 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "## Load Flair Libraries\n",
    "from flair import __version__ as flair_version\n",
    "from flair.splitter import SegtokSentenceSplitter\n",
    "from utils.nlp_flair import run_flair, add_morphosyntax_flair\n",
    "from flair.nn import Classifier\n",
    "\n",
    "splitter = SegtokSentenceSplitter()\n",
    "ner_tagger = Classifier.load('ner-ontonotes-large')\n",
    "rel_tagger = Classifier.load('relations')\n",
    "frames_tagger = Classifier.load('frame')\n",
    "linker_tagger = Classifier.load('linker')\n",
    "\n",
    "flair_models = {\n",
    "    \"chunker\": \"chunk\",\n",
    "    \"ner\": ner_tagger, # These are the specific pre-trained models, can be switched...\n",
    "    \"relations\": rel_tagger,\n",
    "    \"frames\": frames_tagger,\n",
    "    \"linker\": linker_tagger\n",
    "}\n",
    "\n",
    "morpho, tokenized_doc = add_morphosyntax_flair(text, splitter)\n",
    "\n",
    "nlp_dict['tokenization'][f\"flair_{flair_version}\"] = tokenized_doc\n",
    "nlp_dict['morphology'][f\"flair_{flair_version}\"] = morpho"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Named Entities & Relations (Flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 7614.61it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = splitter.split(text)\n",
    "\n",
    "if 'entities' not in nlp_dict: nlp_dict['entities'] = []\n",
    "if 'relations' not in nlp_dict: nlp_dict['relations'] = []\n",
    "\n",
    "ent_rel_out = run_flair(sentences, \"relations\", flair_models)\n",
    "nlp_dict['entities'] = ent_rel_out[\"tagged_entities\"]\n",
    "nlp_dict['relations'] = ent_rel_out[\"tagged_relations\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Linked Entities (Flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 25280.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Must restart the sentence to erase previous tags\n",
    "sentences = splitter.split(text)\n",
    "if 'linked_entities' not in nlp_dict: nlp_dict['linked_entities'] = []\n",
    "nlp_dict['linked_entities'] = run_flair(sentences, \"linker\", flair_models, metadata={\"entity_ids\":ent_rel_out[\"entity_ids\"]})[\"tagged_entities\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Predicate Senses & Merge with SRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 9660.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 120 spell.01\n",
      "172 175 be.01\n",
      "246 250 bear.02\n",
      "271 282 establish.01\n",
      "302 311 influence.01\n",
      "385 388 be.01\n",
      "392 399 contact.01\n",
      "518 521 be.03\n",
      "522 532 patronize.01\n",
      "584 592 include.01\n",
      "609 618 prefer.01\n",
      "737 740 be.01\n",
      "806 813 include.01\n",
      "862 867 die.01\n",
      "966 970 mark.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "frames_flair = run_flair(sentences, \"frames\", flair_models)[\"tagged_entities\"]\n",
    "\n",
    "for fr in frames_flair:\n",
    "    print(fr[\"locationStart\"],fr[\"locationEnd\"],fr[\"predicateSense\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save File Appending the new Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/json/albrecht_dürer.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "intavia_dict = {\n",
    "            'status': '200',\n",
    "            'data': nlp_dict\n",
    "        }\n",
    "print(json_nlp_filename)\n",
    "json.dump(intavia_dict, open(json_nlp_filename, \"w\"), indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
