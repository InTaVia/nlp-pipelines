{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Wikipedia Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.utils_wiki import get_wikipedia_article, save_wikipedia_page, extract_sections\n",
    "\n",
    "person_name = \"Albrecht Duerer\"\n",
    "\n",
    "\n",
    "if not os.path.exists(\"data/wikipedia\"): os.makedirs(\"data/wikipedia\")\n",
    "if not os.path.exists(\"data/json\"): os.makedirs(\"data/json\")\n",
    "text_filename = f\"data/wikipedia/{person_name.replace(' ', '_').lower()}.txt\"\n",
    "json_nlp_filename = f\"data/json/{person_name.replace(' ', '_').lower()}.json\"\n",
    "\n",
    "wiki_page = get_wikipedia_article(person_name)\n",
    "if wiki_page:\n",
    "    print(f\"Found a Page: {wiki_page.title}\")\n",
    "    text = wiki_page.content\n",
    "    section_dict = extract_sections(text)\n",
    "    save_wikipedia_page(wiki_page, text_filename, include_metadata=True, section_dict=section_dict)\n",
    "else:\n",
    "    print(f\"Query Failed! Couldn't find {person_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean & Pre-process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.nlp_common as unlp\n",
    "import spacy\n",
    "from spacy import __version__ as spacy_version\n",
    "\n",
    "person_name = \"Albrecht DÃ¼rer\"\n",
    "text_filename = f\"data/wikipedia/{person_name.replace(' ', '_').lower()}.txt\"\n",
    "json_nlp_filename = f\"data/json/{person_name.replace(' ', '_').lower()}.json\"\n",
    "\n",
    "with open(text_filename) as f:\n",
    "    text = f.read()\n",
    "    text = text[:1000]\n",
    "    text = unlp.preprocess_and_clean_text(text)\n",
    "    nlp_dict, is_from_file = unlp.create_nlp_template(text, filepath=json_nlp_filename)\n",
    "\n",
    "# NLP Basic processing using SpaCy (Only if file did not exist already)\n",
    "if not is_from_file:\n",
    "    spacy_model = \"en_core_web_lg\"\n",
    "    nlp = spacy.load(spacy_model)\n",
    "    spacy_dict = unlp.run_spacy(text, nlp)\n",
    "    sentences = spacy_dict['sentences']\n",
    "    token_objs = spacy_dict['token_objs']\n",
    "    nlp_dict['tokenization'] = {f'spacy_{spacy_model}_{spacy_version}': spacy_dict['tokens']}\n",
    "    nlp_dict['morphology'] = {f'spacy_{spacy_model}_{spacy_version}': unlp.add_morphosyntax(spacy_dict['token_objs'])}\n",
    "else:\n",
    "    sentences, token_objs = [], []\n",
    "    flair_versions = [v for v in nlp_dict['morphology'].keys() if 'flair_' in v]\n",
    "    if len(flair_versions)>0:\n",
    "        for sent_ix, sent_obj in enumerate(nlp_dict['morphology'][flair_versions[0]]):\n",
    "            if len(sent_obj['text']) > 0:\n",
    "                sentences.append(sent_obj['text'])\n",
    "                for tok in sent_obj['words']:\n",
    "                    token_objs.append({'sent_id': sent_ix, 'text': tok['FORM'], 'lemma': tok['FORM'], \n",
    "                                    'start_char': tok['MISC']['StartChar'], 'end_char': tok['MISC']['EndChar'], 'space_after': tok['MISC']['SpaceAfter']})\n",
    "\n",
    "print(text[:100])\n",
    "print(nlp_dict.keys())\n",
    "print(is_from_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Semantic Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nlp_allen import add_json_srl_allennlp\n",
    "from allennlp.predictors import Predictor\n",
    "\n",
    "srl_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")\n",
    "nlp_dict['semantic_roles'] += add_json_srl_allennlp(sentences, srl_predictor, token_objs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Named Entities (AllenNLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nlp_allen import add_json_ner_allennlp\n",
    "\n",
    "ner_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/ner-elmo.2021-02-12.tar.gz\")\n",
    "nlp_dict['entities'] += add_json_ner_allennlp(sentences, ner_predictor, token_objs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Coreferences (AllenNLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nlp_allen import add_json_coref_allennlp\n",
    "\n",
    "coref_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\")\n",
    "nlp_dict['coreference'] = add_json_coref_allennlp(sentences, coref_predictor, token_objs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Time Expressions (HeidelTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_heideltime import Heideltime\n",
    "from utils.nlp_heideltime import add_json_heideltime\n",
    "heideltime_parser = Heideltime()\n",
    "heideltime_parser.set_language('ENGLISH')\n",
    "heideltime_parser.set_document_type('NARRATIVES')\n",
    "\n",
    "nlp_dict['time_expressions'] = add_json_heideltime(text, heideltime_parser)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "intavia_dict = {\n",
    "            'status': '200',\n",
    "            'data': nlp_dict\n",
    "        }\n",
    "\n",
    "json.dump(intavia_dict, open(json_nlp_filename, \"w\"), indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
