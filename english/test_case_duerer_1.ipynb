{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Wikipedia Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: {'Goslar', 'Deutsche Schule Sevilla', 'Albrecht D端rer'}\n",
      "Ordered Options Compund Metric: [RankedArticle(wikipage_title='Albrecht D端rer', queried_name='Albrecht Duerer', lev_similarity=0.896551724137931, token_overlap=0.5, dates_confidence=-1)]\n",
      "\n",
      "Retrieving page for Albrecht D端rer\n",
      "Wiki Life Data = (1471 - 1528)\n",
      "Page Chosen! Confidence Score = 1\n",
      "Found a Page: Albrecht D端rer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from utils.utils_wiki import get_wikipedia_article, save_wikipedia_page, extract_sections\n",
    "\n",
    "person_name = \"Albrecht Duerer\"\n",
    "\n",
    "\n",
    "if not os.path.exists(\"data/wikipedia\"): os.makedirs(\"data/wikipedia\")\n",
    "if not os.path.exists(\"data/json\"): os.makedirs(\"data/json\")\n",
    "text_filename = f\"data/wikipedia/{person_name.replace(' ', '_').lower()}.txt\"\n",
    "json_nlp_filename = f\"data/json/{person_name.replace(' ', '_').lower()}.json\"\n",
    "\n",
    "wiki_page = get_wikipedia_article(person_name)\n",
    "if wiki_page:\n",
    "    print(f\"Found a Page: {wiki_page.title}\")\n",
    "    text = wiki_page.content\n",
    "    section_dict = extract_sections(text)\n",
    "    save_wikipedia_page(wiki_page, text_filename, include_metadata=True, section_dict=section_dict)\n",
    "else:\n",
    "    print(f\"Query Failed! Couldn't find {person_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean & Pre-process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.nlp_common as unlp\n",
    "import spacy\n",
    "from spacy import __version__ as spacy_version\n",
    "\n",
    "\n",
    "with open(text_filename) as f:\n",
    "    text = f.read()\n",
    "    text = text[:1000]\n",
    "    text = unlp.preprocess_and_clean_text(text)\n",
    "    nlp_dict, is_from_file = unlp.create_nlp_template(text, filepath=json_nlp_filename)\n",
    "\n",
    "# NLP Basic processing using SpaCy (Only if file did not exist already)\n",
    "if not is_from_file:\n",
    "    spacy_model = \"en_core_web_sm\"\n",
    "    nlp = spacy.load(spacy_model)\n",
    "    spacy_dict = unlp.run_spacy(text, nlp)\n",
    "    nlp_dict['tokenization'] = {f'spacy_{spacy_model}_{spacy_version}': spacy_dict['tokens']}\n",
    "    nlp_dict['morphology'] = {f'spacy_{spacy_model}_{spacy_version}': unlp.add_morphosyntax(spacy_dict['token_objs'])}\n",
    "else:\n",
    "    spacy_model = \"en_core_web_sm\"\n",
    "    nlp = spacy.load(spacy_model)\n",
    "    text = nlp_dict['text']\n",
    "    spacy_dict = unlp.run_spacy(text, nlp)\n",
    "\n",
    "print(text[:100])\n",
    "print(nlp_dict.keys())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Semantic Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nlp_allen import add_json_srl_allennlp\n",
    "from allennlp.predictors import Predictor\n",
    "\n",
    "srl_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")\n",
    "nlp_dict['semantic_roles'] += add_json_srl_allennlp(spacy_dict['sentences'], srl_predictor, spacy_dict['token_objs'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Named Entities (AllenNLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nlp_allen import add_json_ner_allennlp\n",
    "\n",
    "ner_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/ner-elmo.2021-02-12.tar.gz\")\n",
    "nlp_dict['entities'] += add_json_ner_allennlp(spacy_dict['sentences'], ner_predictor, spacy_dict['token_objs'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Correferences (AllenNLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nlp_allen import add_json_coref_allennlp\n",
    "\n",
    "coref_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\")\n",
    "nlp_dict['coreference'] = add_json_coref_allennlp(spacy_dict['sentences'], coref_predictor, spacy_dict['token_objs'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Time Expressions (HeidelTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_heideltime import Heideltime\n",
    "from utils.nlp_heideltime import add_json_heideltime\n",
    "heideltime_parser = Heideltime()\n",
    "heideltime_parser.set_language('ENGLISH')\n",
    "heideltime_parser.set_document_type('NARRATIVES')\n",
    "\n",
    "nlp_dict['time_expressions'] = add_json_heideltime(text, heideltime_parser)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "intavia_dict = {\n",
    "            'status': '200',\n",
    "            'data': nlp_dict\n",
    "        }\n",
    "\n",
    "json.dump(intavia_dict, open(json_nlp_filename, \"w\"), indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
