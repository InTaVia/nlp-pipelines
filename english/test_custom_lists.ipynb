{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing a Text File to Find Wikipedia Links\n",
    "\n",
    "The case when we have also Birth and Death dates to help improve disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def pre_process_list(filepath: str):\n",
    "    with open(filepath) as f:\n",
    "        rows = f.readlines()\n",
    "\n",
    "    life_pattern = r\"(\\d+)\\s+(â€“|-)\\s+(\\d+)\"\n",
    "    people = []\n",
    "    for row in rows:\n",
    "        if '(' in row and ')' in row:\n",
    "            bracket_open_index = row.index('(')\n",
    "            bracket_closed_index = row.index(')')\n",
    "            name = row[:bracket_open_index].strip()\n",
    "            lifespan = row[bracket_open_index:bracket_closed_index+1].strip()\n",
    "            if len(row) > bracket_closed_index+1:\n",
    "                description = row[bracket_closed_index+1:].strip()\n",
    "            else:\n",
    "                description = \"\"\n",
    "            m = re.search(life_pattern, lifespan)\n",
    "            if m:\n",
    "                birth = m.group(1).strip()\n",
    "                death = m.group(3).strip()\n",
    "            else:\n",
    "                birth = -1\n",
    "                death = -1\n",
    "            people.append((name, int(birth), int(death)))\n",
    "            # print(\"Name:\", name)\n",
    "            # print(\"Lifespan:\", lifespan, birth, death)\n",
    "            # print(\"Description:\", description)\n",
    "            # print(\"---\")\n",
    "        else:\n",
    "            print(\"MISSED\", row)\n",
    "    \n",
    "    return people\n",
    "\n",
    "def pre_process_latinamerica_list(filepath: str):\n",
    "    with open(filepath) as f:\n",
    "        rows = f.readlines()\n",
    "    people = []\n",
    "    countries = []\n",
    "    for row in rows:\n",
    "        columns = row.split('|')\n",
    "        name = columns[0].strip()\n",
    "        country = columns[1].strip()\n",
    "        countries.append(country)\n",
    "        birth = -1\n",
    "        death = -1\n",
    "        people.append((name, int(birth), int(death)))\n",
    "    \n",
    "    [print(x) for x in Counter(countries).most_common()]\n",
    "    return people\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: {'Moctezuma I', 'Isabel Moctezuma', 'Moctezuma II'}\n",
      "Ordered Options Compund Metric: [RankedArticle(wikipage_title='Moctezuma II', queried_name='Moctezuma II', lev_similarity=1.0, token_overlap=1.0, dates_confidence=-1), RankedArticle(wikipage_title='Moctezuma I', queried_name='Moctezuma II', lev_similarity=0.9565217391304348, token_overlap=0.5, dates_confidence=-1), RankedArticle(wikipage_title='Isabel Moctezuma', queried_name='Moctezuma II', lev_similarity=0.6428571428571428, token_overlap=0.5, dates_confidence=-1)]\n",
      "\n",
      "Retrieving page for Moctezuma II\n",
      "Wiki Life Data = (1460 - 1520)\n",
      "Page Chosen! Confidence Score = 1\n",
      "Options: {'Malinche (volcano)', 'La Malinche', 'La Llorona'}\n",
      "Ordered Options Compund Metric: [RankedArticle(wikipage_title='La Malinche', queried_name='La Malinche', lev_similarity=1.0, token_overlap=1.0, dates_confidence=-1), RankedArticle(wikipage_title='Malinche (volcano)', queried_name='La Malinche', lev_similarity=0.5517241379310345, token_overlap=0.5, dates_confidence=-1), RankedArticle(wikipage_title='La Llorona', queried_name='La Malinche', lev_similarity=0.47619047619047616, token_overlap=0.5, dates_confidence=-1)]\n",
      "\n",
      "Retrieving page for La Malinche\n",
      "Wiki Life Data = (1490 - 1520)\n",
      "Page Chosen! Confidence Score = 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/daza/ReposPublic/nlp-pipelines/english/test_custom_lists.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daza/ReposPublic/nlp-pipelines/english/test_custom_lists.ipynb#W2sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         exit()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daza/ReposPublic/nlp-pipelines/english/test_custom_lists.ipynb#W2sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# women_famous = pre_process_list(\"resources/15_famous_women.txt\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daza/ReposPublic/nlp-pipelines/english/test_custom_lists.ipynb#W2sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# create_ready_filelist(women_famous, \"resources/15_famous_women.ready.txt\", \"data/wikipedia/top_women\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daza/ReposPublic/nlp-pipelines/english/test_custom_lists.ipynb#W2sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daza/ReposPublic/nlp-pipelines/english/test_custom_lists.ipynb#W2sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# get_pages_from_ready(\"resources/12_activists_lgbtq.ready.txt\", \"data/wikipedia/top_lgbtq\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/daza/ReposPublic/nlp-pipelines/english/test_custom_lists.ipynb#W2sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m get_pages_from_ready(\u001b[39m\"\u001b[39;49m\u001b[39mresources/50_famous_mexico.ready.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mdata/wikipedia/top_mexico\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/daza/ReposPublic/nlp-pipelines/english/test_custom_lists.ipynb Cell 3\u001b[0m in \u001b[0;36mget_pages_from_ready\u001b[0;34m(filepath, output_dir)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daza/ReposPublic/nlp-pipelines/english/test_custom_lists.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mif\u001b[39;00m page:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daza/ReposPublic/nlp-pipelines/english/test_custom_lists.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     save_wikipedia_page(page, output_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00moutput_dir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mpage\u001b[39m.\u001b[39mtitle\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mlower()\u001b[39m}\u001b[39;00m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m, include_metadata\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, include_sections\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, include_infobox\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/daza/ReposPublic/nlp-pipelines/english/test_custom_lists.ipynb#W2sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m5\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daza/ReposPublic/nlp-pipelines/english/test_custom_lists.ipynb#W2sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m exit()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from utils.utils_wiki import get_wikipedia_article, save_wikipedia_page\n",
    "import time, os\n",
    "from typing import List, Tuple\n",
    "from urllib.parse import unquote\n",
    "\n",
    "def create_ready_filelist(people: List[Tuple], output_list: str, output_dir: str):\n",
    "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "    with open(output_list, \"w\") as f:\n",
    "        for name, birth, death in people:\n",
    "            page = get_wikipedia_article(name, query_restrictions={'birth_year': birth, 'death_year': death})\n",
    "            if page:\n",
    "                save_wikipedia_page(page, output_path=f\"{output_dir}/{page.title.replace(' ', '_').lower()}.txt\", include_metadata=True, include_sections=True, include_infobox=True)\n",
    "                f.write(f\"{page.title} | {page.url}\\n\")\n",
    "            time.sleep(5)\n",
    "\n",
    "def get_person_name(wiki_link: str):\n",
    "    if \"/\" not in wiki_link: return None\n",
    "    name_url = wiki_link.split(\"/\")[-1]\n",
    "    person_name = unquote(name_url)\n",
    "    return person_name.replace(\"_\", \" \")\n",
    "\n",
    "def get_pages_from_ready(filepath: str, output_dir: str):\n",
    "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "    wiki_titles = []\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            elems = line.split(\"|\")\n",
    "            person_wiki_name = get_person_name(elems[1].strip())\n",
    "            wiki_titles.append(person_wiki_name)\n",
    "    for title in wiki_titles:\n",
    "        page = get_wikipedia_article(title)\n",
    "        if page:\n",
    "            save_wikipedia_page(page, output_path=f\"{output_dir}/{page.title.replace(' ', '_').lower()}.txt\", include_metadata=True, include_sections=True, include_infobox=True)\n",
    "        time.sleep(5)\n",
    "\n",
    "# women_famous = pre_process_list(\"resources/15_famous_women.txt\")\n",
    "# create_ready_filelist(women_famous, \"resources/15_famous_women.ready.txt\", \"data/wikipedia/top_women\")\n",
    "\n",
    "# get_pages_from_ready(\"resources/12_activists_lgbtq.ready.txt\", \"data/wikipedia/top_lgbtq\")\n",
    "\n",
    "# get_pages_from_ready(\"resources/50_famous_mexico.ready.txt\", \"data/wikipedia/top_mexico\")\n",
    "\n",
    "# top_100_people = pre_process_list(\"resources/top_100_world_most_influential.txt\")\n",
    "# create_ready_filelist(top_100_people, \"resources/top_100_world_most_influential.ready.txt\", \"data/wikipedia/top100\")\n",
    "\n",
    "get_pages_from_ready(\"resources/100_famous_latinamerica.ready.txt\", \"data/wikipedia/top_latinamerica\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing a Wikipedia Page with a Table\n",
    "\n",
    "The case of the \"List of women explorers and travelers\" [Link](https://en.wikipedia.org/wiki/List_of_women_explorers_and_travelers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
