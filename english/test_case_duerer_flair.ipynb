{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Wikipedia Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.utils_wiki import get_wikipedia_article, save_wikipedia_page\n",
    "from utils.nlp_common import preprocess_and_clean_text\n",
    "\n",
    "person_name = \"Albrecht Duerer\"\n",
    "\n",
    "# person_name = \"Ida Pfeiffer\" # To test for any name on demand ...\n",
    "\n",
    "wikipedia_title = person_name\n",
    "\n",
    "\n",
    "if not os.path.exists(\"data/wikipedia\"): os.makedirs(\"data/wikipedia\")\n",
    "if not os.path.exists(\"data/json\"): os.makedirs(\"data/json\")\n",
    "\n",
    "\n",
    "wiki_page = get_wikipedia_article(person_name)\n",
    "if wiki_page:\n",
    "    print(f\"Found a Page: {wiki_page.title}\")\n",
    "    text = wiki_page.content\n",
    "    wikipedia_title = wiki_page.title\n",
    "    text_filename = f\"data/wikipedia/{wikipedia_title.replace(' ', '_').lower()}.txt\"\n",
    "    json_nlp_filename = f\"data/json/{wikipedia_title.replace(' ', '_').lower()}.json\"\n",
    "    save_wikipedia_page(wiki_page, text_filename, include_metadata=True, include_sections=True)\n",
    "else:\n",
    "    print(f\"Query Failed! Couldn't find {person_name}\")\n",
    "\n",
    "text = preprocess_and_clean_text(text)\n",
    "print(wikipedia_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from collections import OrderedDict\n",
    "from typing import  Dict, Optional, Any\n",
    "from utils.utils_wiki import get_wiki_linked_entities\n",
    "\n",
    "text_filename = f\"data/wikipedia/{wikipedia_title.replace(' ', '_').lower()}.txt\"\n",
    "json_nlp_filename = f\"data/json/{wikipedia_title.replace(' ', '_').lower()}.json\"\n",
    "\n",
    "if wikipedia_title:\n",
    "    raw_file = f\"data/wikipedia/{wikipedia_title.replace(' ', '_').lower()}.raw.txt\"\n",
    "    response = requests.get(f'https://en.wikipedia.org/wiki/{wikipedia_title}?action=raw')\n",
    "    raw_wiki = response.text\n",
    "    with open(raw_file, \"w\") as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "linked = get_wiki_linked_entities(raw_wiki)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean & Pre-process Text (SpaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nlp_common import create_nlp_template, add_morphosyntax, run_spacy, preprocess_and_clean_text\n",
    "import spacy\n",
    "from spacy import __version__ as spacy_version\n",
    "\n",
    "\n",
    "with open(text_filename) as f:\n",
    "    text = f.read()\n",
    "    text = text[:5000]\n",
    "    text = preprocess_and_clean_text(text)\n",
    "    nlp_dict, is_from_file = create_nlp_template(text, filepath=json_nlp_filename)\n",
    "\n",
    "# NLP Basic processing using SpaCy (Only if file did not exist already)\n",
    "if not is_from_file:\n",
    "    spacy_model = \"en_core_web_sm\"\n",
    "    nlp = spacy.load(spacy_model)\n",
    "    spacy_dict = run_spacy(text, nlp)\n",
    "    nlp_dict['tokenization'] = {f'spacy_{spacy_model}_{spacy_version}': spacy_dict['tokens']}\n",
    "    nlp_dict['morphology'] = {f'spacy_{spacy_model}_{spacy_version}': add_morphosyntax(spacy_dict['token_objs'])}\n",
    "else:\n",
    "    text = nlp_dict['text']\n",
    "\n",
    "print(text[:100])\n",
    "print(nlp_dict.keys())\n",
    "print(nlp_dict[\"entities\"])\n",
    "print(is_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Flair Libraries\n",
    "from flair import __version__ as flair_version\n",
    "from flair.splitter import SegtokSentenceSplitter\n",
    "from utils.nlp_flair import run_flair, add_morphosyntax_flair\n",
    "from flair.nn import Classifier\n",
    "\n",
    "splitter = SegtokSentenceSplitter()\n",
    "ner_tagger = Classifier.load('ner-ontonotes-large')\n",
    "rel_tagger = Classifier.load('relations')\n",
    "frames_tagger = Classifier.load('frame')\n",
    "linker_tagger = Classifier.load('linker')\n",
    "\n",
    "flair_models = {\n",
    "    \"chunker\": \"chunk\",\n",
    "    \"ner\": ner_tagger, # These are the specific pre-trained models, can be switched...\n",
    "    \"relations\": rel_tagger,\n",
    "    \"frames\": frames_tagger,\n",
    "    \"linker\": linker_tagger\n",
    "}\n",
    "\n",
    "morpho, tokenized_doc = add_morphosyntax_flair(text, splitter)\n",
    "\n",
    "nlp_dict['tokenization'][f\"flair_{flair_version}\"] = tokenized_doc\n",
    "nlp_dict['morphology'][f\"flair_{flair_version}\"] = morpho"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Named Entities & Relations (Flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = splitter.split(text)\n",
    "\n",
    "if 'entities' not in nlp_dict: nlp_dict['entities'] = []\n",
    "if 'relations' not in nlp_dict: nlp_dict['relations'] = []\n",
    "\n",
    "ent_rel_out = run_flair(sentences, \"relations\", flair_models)\n",
    "nlp_dict['entities'] = ent_rel_out[\"tagged_entities\"]\n",
    "nlp_dict['relations'] = ent_rel_out[\"tagged_relations\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Linked Entities (Flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must restart the sentence to erase previous tags\n",
    "sentences = splitter.split(text)\n",
    "if 'linked_entities' not in nlp_dict: nlp_dict['linked_entities'] = []\n",
    "nlp_dict['linked_entities'] = run_flair(sentences, \"linker\", flair_models, metadata={\"entity_ids\":ent_rel_out[\"entity_ids\"]})[\"tagged_entities\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Predicate Senses & Merge with SRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_flair = run_flair(sentences, \"frames\", flair_models)[\"tagged_entities\"]\n",
    "\n",
    "for fr in frames_flair:\n",
    "    print(fr[\"locationStart\"],fr[\"locationEnd\"],fr[\"predicateSense\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save File Appending the new Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "intavia_dict = {\n",
    "            'status': '200',\n",
    "            'data': nlp_dict\n",
    "        }\n",
    "print(json_nlp_filename)\n",
    "print(nlp_dict)\n",
    "json.dump(intavia_dict, open(json_nlp_filename, \"w\"), indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
