{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Wikipedia Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: {'Albrecht Dürer', 'Deutsche Schule Sevilla', 'Erasmus'}\n",
      "Ordered Options Compund Metric: [RankedArticle(wikipage_title='Albrecht Dürer', queried_name='Albrecht Duerer', lev_similarity=0.896551724137931, token_overlap=0.5, dates_confidence=-1)]\n",
      "\n",
      "Retrieving page for Albrecht Dürer\n",
      "Wiki Life Data = (1471 - 1528)\n",
      "Page Chosen! Confidence Score = 1\n",
      "Found a Page: Albrecht Dürer\n",
      "Albrecht Dürer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from utils.utils_wiki import get_wikipedia_article, save_wikipedia_page\n",
    "from utils.nlp_common import preprocess_and_clean_text\n",
    "\n",
    "person_name = \"Albrecht Duerer\"\n",
    "\n",
    "# person_name = \"Ida Pfeiffer\" # To test for any name on demand ...\n",
    "\n",
    "wikipedia_title = person_name\n",
    "\n",
    "\n",
    "if not os.path.exists(\"data/wikipedia\"): os.makedirs(\"data/wikipedia\")\n",
    "if not os.path.exists(\"data/json\"): os.makedirs(\"data/json\")\n",
    "\n",
    "\n",
    "wiki_page = get_wikipedia_article(person_name)\n",
    "if wiki_page:\n",
    "    print(f\"Found a Page: {wiki_page.title}\")\n",
    "    text = wiki_page.content\n",
    "    wikipedia_title = wiki_page.title\n",
    "    text_filename = f\"data/wikipedia/{wikipedia_title.replace(' ', '_').lower()}.txt\"\n",
    "    json_nlp_filename = f\"data/json/{wikipedia_title.replace(' ', '_').lower()}.json\"\n",
    "    save_wikipedia_page(wiki_page, text_filename, include_metadata=True, include_sections=True)\n",
    "else:\n",
    "    print(f\"Query Failed! Couldn't find {person_name}\")\n",
    "\n",
    "text = preprocess_and_clean_text(text)\n",
    "print(wikipedia_title)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean & Pre-process Text (SpaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albrecht Dürer (; German: [ˈʔalbʁɛçt ˈdyːʁɐ]; 21 May 1471 – 6 April 1528), sometimes spelled in Engl\n",
      "dict_keys(['text', 'tokenization', 'morpho_syntax', 'entities', 'time_expressions', 'semantic_roles', 'coreference'])\n",
      "[]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from utils.nlp_common import create_nlp_template, add_morphosyntax, run_spacy, preprocess_and_clean_text\n",
    "import spacy\n",
    "from spacy import __version__ as spacy_version\n",
    "\n",
    "\n",
    "with open(text_filename) as f:\n",
    "    text = f.read()\n",
    "    text = preprocess_and_clean_text(text)\n",
    "    nlp_dict, is_from_file = create_nlp_template(text, filepath=json_nlp_filename)\n",
    "\n",
    "# NLP Basic processing using SpaCy (Only if file did not exist already)\n",
    "if not is_from_file:\n",
    "    spacy_model = \"en_core_web_sm\"\n",
    "    nlp = spacy.load(spacy_model)\n",
    "    spacy_dict = run_spacy(text, nlp)\n",
    "    nlp_dict['tokenization'] = {f'spacy_{spacy_model}_{spacy_version}': spacy_dict['tokens']}\n",
    "    nlp_dict['morpho_syntax'] = {f'spacy_{spacy_model}_{spacy_version}': add_morphosyntax(spacy_dict['token_objs'])}\n",
    "else:\n",
    "    text = nlp_dict['text']\n",
    "\n",
    "print(text[:100])\n",
    "print(nlp_dict.keys())\n",
    "print(nlp_dict[\"entities\"])\n",
    "print(is_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daza/anaconda3/envs/test_pipelines/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-18 16:22:19,443 SequenceTagger predicts: Dictionary with 76 tags: <unk>, O, B-CARDINAL, E-CARDINAL, S-PERSON, S-CARDINAL, S-PRODUCT, B-PRODUCT, I-PRODUCT, E-PRODUCT, B-WORK_OF_ART, I-WORK_OF_ART, E-WORK_OF_ART, B-PERSON, E-PERSON, S-GPE, B-DATE, I-DATE, E-DATE, S-ORDINAL, S-LANGUAGE, I-PERSON, S-EVENT, S-DATE, B-QUANTITY, E-QUANTITY, S-TIME, B-TIME, I-TIME, E-TIME, B-GPE, E-GPE, S-ORG, I-GPE, S-NORP, B-FAC, I-FAC, E-FAC, B-NORP, E-NORP, S-PERCENT, B-ORG, E-ORG, B-LANGUAGE, E-LANGUAGE, I-CARDINAL, I-ORG, S-WORK_OF_ART, I-QUANTITY, B-MONEY\n",
      "2023-08-18 16:22:25,151 SequenceTagger predicts: Dictionary with 4852 tags: <unk>, be.01, be.03, have.01, say.01, do.01, have.03, do.02, be.02, know.01, think.01, come.01, see.01, want.01, go.02, tell.01, give.01, use.01, make.02, take.01, talk.01, get.01, go.04, live.01, need.01, believe.01, work.01, mean.01, have.02, look.01, become.01, die.01, help.01, find.01, try.01, hear.01, know.06, show.01, happen.01, let.01, sell.01, bring.01, make.01, invest.01, begin.01, make.LV, continue.01, kill.01, speak.01, start.01\n",
      "2023-08-18 16:22:48,192 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "## Load Flair Libraries\n",
    "from flair import __version__ as flair_version\n",
    "from flair.splitter import SegtokSentenceSplitter\n",
    "from utils.nlp_flair import run_flair, add_morphosyntax_flair\n",
    "from flair.nn import Classifier\n",
    "\n",
    "splitter = SegtokSentenceSplitter()\n",
    "ner_tagger = Classifier.load('ner-ontonotes-large')\n",
    "rel_tagger = Classifier.load('relations')\n",
    "frames_tagger = Classifier.load('frame')\n",
    "linker_tagger = Classifier.load('linker')\n",
    "\n",
    "flair_models = {\n",
    "    \"chunker\": \"chunk\",\n",
    "    \"ner\": ner_tagger, # These are the specific pre-trained models, can be switched...\n",
    "    \"relations\": rel_tagger,\n",
    "    \"frames\": frames_tagger,\n",
    "    \"linker\": linker_tagger\n",
    "}\n",
    "\n",
    "morpho, tokenized_doc = add_morphosyntax_flair(text, splitter)\n",
    "\n",
    "nlp_dict['tokenization'][f\"flair_{flair_version}\"] = tokenized_doc\n",
    "nlp_dict['morpho_syntax'][f\"flair_{flair_version}\"] = morpho"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Named Entities & Relations (Flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 9246.82it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = splitter.split(text)\n",
    "\n",
    "if 'entities' not in nlp_dict: nlp_dict['entities'] = []\n",
    "if 'relations' not in nlp_dict: nlp_dict['relations'] = []\n",
    "\n",
    "ent_rel_out = run_flair(sentences, \"relations\", flair_models)\n",
    "nlp_dict['entities'] = ent_rel_out[\"tagged_entities\"]\n",
    "nlp_dict['relations'] = ent_rel_out[\"tagged_relations\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Linked Entities (Flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 29745.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Must restart the sentence to erase previous tags\n",
    "sentences = splitter.split(text)\n",
    "if 'linked_entities' not in nlp_dict: nlp_dict['linked_entities'] = []\n",
    "nlp_dict['linked_entities'] = run_flair(sentences, \"linker\", flair_models, metadata={\"entity_ids\":ent_rel_out[\"entity_ids\"]})[\"tagged_entities\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Predicate Senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 16873.07it/s]\n"
     ]
    }
   ],
   "source": [
    "frames_flair = run_flair(sentences, \"frames\", flair_models)[\"tagged_entities\"]\n",
    "nlp_dict['frames'] = frames_flair"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save File Appending the new Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/json/albrecht_dürer.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "intavia_dict = {\n",
    "            'status': '200',\n",
    "            'data': nlp_dict\n",
    "        }\n",
    "print(json_nlp_filename)\n",
    "json.dump(intavia_dict, open(json_nlp_filename, \"w\"), indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
